#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Scraper Intelligent

Ce script effectue un scraping intelligent en √©vitant de re-scraper
les fichiers d√©j√† pr√©sents dans la base de donn√©es.

Auteur: Assistant IA
Date: 2025-01-27
"""

import os
import sys
import sqlite3
import subprocess
import time
import logging
from typing import Set, List, Dict
from pathlib import Path

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class IntelligentScraper:
    """
    Scraper intelligent qui √©vite les doublons
    """
    
    def __init__(self):
        self.db_path = 'db.sqlite3'
        self.sites = {
            'agriculture.gov.ma': 'agriculture_scraper.py',
            'bkam.ma': 'bkam_scraper.py',
            'cese.ma': 'cese_scraper.py', 
            'finances.gov.ma': 'finances_scraper.py',
            'oecd.org': 'oecd_scraper.py'
        }
        
    def get_existing_files_from_db(self) -> Set[str]:
        """
        R√©cup√®re les fichiers d√©j√† pr√©sents dans la base de donn√©es
        
        Returns:
            Set[str]: Ensemble des noms de fichiers d√©j√† scrap√©s
        """
        try:
            if not os.path.exists(self.db_path):
                logger.info("üìä Base de donn√©es non trouv√©e - tous les fichiers seront scrap√©s")
                return set()
                
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT DISTINCT file_path 
                FROM scraper_scrapedpdf
            """)
            
            existing_files = set()
            for row in cursor.fetchall():
                file_path = row[0]
                filename = os.path.basename(file_path)
                existing_files.add(filename)
            
            conn.close()
            logger.info(f"üìä {len(existing_files)} fichiers d√©j√† pr√©sents dans la base de donn√©es")
            return existing_files
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la lecture de la base de donn√©es: {e}")
            return set()
    
    def get_files_in_directory(self, directory: str) -> List[str]:
        """
        R√©cup√®re la liste des fichiers PDF dans un r√©pertoire
        
        Args:
            directory (str): Chemin du r√©pertoire
        
        Returns:
            List[str]: Liste des fichiers PDF
        """
        pdf_files = []
        
        if not os.path.exists(directory):
            return pdf_files
        
        try:
            for root, dirs, files in os.walk(directory):
                for file in files:
                    if file.lower().endswith('.pdf'):
                        pdf_files.append(file)
            return pdf_files
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la lecture du r√©pertoire {directory}: {e}")
            return pdf_files
    
    def check_scraping_needed(self, site: str) -> bool:
        """
        V√©rifie si le scraping est n√©cessaire pour un site donn√©
        
        Args:
            site (str): Nom du site
        
        Returns:
            bool: True si scraping n√©cessaire
        """
        existing_files = self.get_existing_files_from_db()
        
        # V√©rifier les fichiers dans le r√©pertoire du site
        pdf_downloads_dir = os.path.join(site, 'pdf_downloads')
        current_files = set(self.get_files_in_directory(pdf_downloads_dir))
        
        # Fichiers qui ne sont pas encore en base
        new_files = current_files - existing_files
        
        if new_files:
            logger.info(f"üÜï {len(new_files)} nouveaux fichiers d√©tect√©s pour {site}")
            for file in sorted(list(new_files)[:3]):  # Afficher max 3 exemples
                logger.info(f"   - {file}")
            if len(new_files) > 3:
                logger.info(f"   ... et {len(new_files) - 3} autres")
            return True
        else:
            logger.info(f"‚úÖ Aucun nouveau fichier pour {site} - scraping ignor√©")
            return False
    
    def run_scraper_for_site(self, site: str, scraper_script: str) -> bool:
        """
        Ex√©cute le scraper pour un site sp√©cifique
        
        Args:
            site (str): Nom du site
            scraper_script (str): Nom du script scraper
        
        Returns:
            bool: True si succ√®s
        """
        try:
            # Chercher le script dans le r√©pertoire du site
            script_path = os.path.join(site, scraper_script)
            
            if not os.path.exists(script_path):
                logger.warning(f"‚ö†Ô∏è  Script {scraper_script} non trouv√© pour {site}")
                return False
            
            logger.info(f"üöÄ D√©marrage du scraping pour {site}...")
            
            # Ex√©cuter le scraper avec un timeout
            result = subprocess.run(
                ['python', script_path],
                cwd=site,  # Ex√©cuter dans le r√©pertoire du site
                check=True,
                capture_output=True,
                text=True,
                timeout=1200  # 20 minutes max par site
            )
            
            logger.info(f"‚úÖ Scraping termin√© avec succ√®s pour {site}")
            
            # Afficher quelques lignes de sortie si disponibles
            if result.stdout.strip():
                lines = result.stdout.strip().split('\n')
                for line in lines[-3:]:  # Derni√®res 3 lignes
                    if line.strip():
                        logger.info(f"   {line}")
            
            return True
            
        except subprocess.TimeoutExpired:
            logger.error(f"‚ùå Timeout du scraping pour {site} (>20 min)")
            return False
            
        except subprocess.CalledProcessError as e:
            logger.error(f"‚ùå Erreur lors du scraping de {site}:")
            logger.error(f"   Code de retour: {e.returncode}")
            
            if e.stderr:
                logger.error(f"   Erreur: {e.stderr.strip()[:200]}...")  # Limiter la sortie
            
            return False
            
        except Exception as e:
            logger.error(f"‚ùå Erreur inattendue lors du scraping de {site}: {e}")
            return False
    
    def run_intelligent_scraping(self):
        """
        Ex√©cute le scraping intelligent pour tous les sites
        """
        logger.info("\n" + "="*60)
        logger.info("üß† SCRAPING INTELLIGENT - √âVITEMENT DES DOUBLONS")
        logger.info("="*60)
        
        successful_sites = 0
        total_sites = len(self.sites)
        
        for site, scraper_script in self.sites.items():
            logger.info(f"\nüîç V√©rification du site: {site}")
            logger.info("-" * 40)
            
            # V√©rifier si le scraping est n√©cessaire
            if self.check_scraping_needed(site):
                # Ex√©cuter le scraper
                if self.run_scraper_for_site(site, scraper_script):
                    successful_sites += 1
                    logger.info(f"‚úÖ Scraping r√©ussi pour {site}")
                else:
                    logger.warning(f"‚ö†Ô∏è  Scraping √©chou√© pour {site}")
            else:
                # Pas de nouveau fichier, consid√©rer comme succ√®s
                successful_sites += 1
                logger.info(f"‚úÖ Aucun scraping n√©cessaire pour {site}")
            
            # Petite pause entre les sites
            if site != list(self.sites.keys())[-1]:  # Pas de pause apr√®s le dernier site
                time.sleep(2)
        
        # R√©sum√©
        logger.info("\n" + "="*60)
        logger.info("üìä R√âSUM√â DU SCRAPING INTELLIGENT")
        logger.info("="*60)
        logger.info(f"‚úÖ Sites trait√©s avec succ√®s: {successful_sites}/{total_sites}")
        
        if successful_sites == total_sites:
            logger.info("üéâ SCRAPING INTELLIGENT TERMIN√â AVEC SUCC√àS !")
            logger.info("   Tous les sites ont √©t√© trait√©s intelligemment.")
        elif successful_sites > 0:
            logger.info("‚ö†Ô∏è  SCRAPING PARTIELLEMENT R√âUSSI")
            logger.info(f"   {total_sites - successful_sites} site(s) ont √©chou√©.")
        else:
            logger.info("‚ùå SCRAPING √âCHOU√â")
            logger.info("   Aucun site n'a pu √™tre trait√©.")
        
        logger.info("="*60)
        
        return successful_sites > 0

def main():
    """
    Fonction principale
    """
    try:
        scraper = IntelligentScraper()
        success = scraper.run_intelligent_scraping()
        
        if success:
            logger.info("\n‚úÖ Scraping intelligent termin√©")
            sys.exit(0)
        else:
            logger.error("\n‚ùå Scraping intelligent √©chou√©")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.info("\n\n‚ö†Ô∏è  Scraping interrompu par l'utilisateur (Ctrl+C)")
        sys.exit(0)
        
    except Exception as e:
        logger.error(f"\n\n‚ùå ERREUR CRITIQUE DU SCRAPING INTELLIGENT !")
        logger.error(f"   Type: {type(e).__name__}")
        logger.error(f"   Message: {str(e)}")
        sys.exit(1)

if __name__ == '__main__':
    main()
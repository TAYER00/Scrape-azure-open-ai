#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de v√©rification de l'analyse des PDFs

Ce script v√©rifie :
1. Que la table scraper_wordsdocument a √©t√© nettoy√©e
2. Que les champs site_web ont √©t√© mis √† jour
3. Les statistiques des r√©sultats d'analyse

Auteur: Assistant IA
Date: 2025-07-25
"""

import sqlite3
import json
import os
from collections import Counter

def verify_database_cleanup():
    """
    V√©rifie l'√©tat de la base de donn√©es apr√®s nettoyage
    """
    print("üîç V√©rification de l'√©tat de la base de donn√©es")
    print("=" * 50)
    
    try:
        conn = sqlite3.connect('db.sqlite3')
        cursor = conn.cursor()
        
        # V√©rifier la table scraper_wordsdocument
        print("\nüìã Table scraper_wordsdocument:")
        cursor.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name='scraper_wordsdocument'
        """)
        
        if cursor.fetchone():
            cursor.execute("SELECT COUNT(*) FROM scraper_wordsdocument")
            count = cursor.fetchone()[0]
            print(f"   Enregistrements: {count}")
            if count == 0:
                print("   ‚úÖ Table correctement nettoy√©e")
            else:
                print(f"   ‚ö†Ô∏è {count} enregistrements restants")
        else:
            print("   ‚ÑπÔ∏è Table non trouv√©e")
        
        # V√©rifier les champs site_web dans scraper_scrapedpdf
        print("\nüìã Table scraper_scrapedpdf - Champs site_web:")
        cursor.execute("""
            SELECT 
                COUNT(*) as total,
                COUNT(site_web) as with_site_web,
                COUNT(website_id) as with_website_id
            FROM scraper_scrapedpdf
        """)
        
        total, with_site_web, with_website_id = cursor.fetchone()
        print(f"   Total PDFs: {total}")
        print(f"   Avec site_web: {with_site_web}")
        print(f"   Avec website_id: {with_website_id}")
        
        if with_site_web == with_website_id:
            print("   ‚úÖ Tous les champs site_web sont mis √† jour")
        else:
            print(f"   ‚ö†Ô∏è {with_website_id - with_site_web} champs site_web manquants")
        
        # Statistiques par site
        print("\nüìä R√©partition par site:")
        cursor.execute("""
            SELECT site_web, COUNT(*) as count
            FROM scraper_scrapedpdf
            WHERE site_web IS NOT NULL
            GROUP BY site_web
            ORDER BY count DESC
        """)
        
        for site, count in cursor.fetchall():
            print(f"   {site}: {count} PDFs")
        
        conn.close()
        
    except Exception as e:
        print(f"‚ùå Erreur v√©rification base de donn√©es: {e}")

def analyze_results_file(filename='scraper/static/data/pdf_analysis_results.json'):
    """
    Analyse les r√©sultats de l'analyse des PDFs
    
    Args:
        filename (str): Nom du fichier de r√©sultats
    """
    print(f"\nüîç Analyse du fichier {filename}")
    print("=" * 50)
    
    if not os.path.exists(filename):
        print(f"‚ùå Fichier {filename} non trouv√©")
        return
    
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            results = json.load(f)
        
        print(f"üìä Total de PDFs analys√©s: {len(results)}")
        
        # Statistiques de pertinence
        pertinents = [r for r in results if r['analysis']['pertinent']]
        non_pertinents = [r for r in results if not r['analysis']['pertinent']]
        
        print(f"‚úÖ PDFs pertinents: {len(pertinents)} ({len(pertinents)/len(results)*100:.1f}%)")
        print(f"‚õî PDFs non pertinents: {len(non_pertinents)} ({len(non_pertinents)/len(results)*100:.1f}%)")
        
        # Statistiques par langue
        print("\nüåç R√©partition par langue:")
        langues = Counter(r['analysis']['langue'] for r in results)
        for langue, count in langues.most_common():
            print(f"   {langue}: {count} PDFs ({count/len(results)*100:.1f}%)")
        
        # Statistiques par th√©matique
        print("\nüè∑Ô∏è R√©partition par th√©matique:")
        thematiques = Counter(r['analysis']['thematique'] for r in results)
        for theme, count in thematiques.most_common(10):  # Top 10
            print(f"   {theme}: {count} PDFs ({count/len(results)*100:.1f}%)")
        
        # Statistiques par site web
        print("\nüåê R√©partition par site web:")
        sites = Counter(r['site_web'] for r in results if r['site_web'])
        for site, count in sites.most_common():
            print(f"   {site}: {count} PDFs ({count/len(results)*100:.1f}%)")
        
        # Statistiques de confiance
        print("\nüéØ Niveau de confiance:")
        confiances = Counter(r['analysis']['confiance'] for r in results)
        for confiance, count in confiances.most_common():
            print(f"   {confiance}: {count} PDFs ({count/len(results)*100:.1f}%)")
        
        # Exemples de PDFs pertinents par th√©matique
        print("\nüìã Exemples de PDFs pertinents par th√©matique:")
        pertinents_by_theme = {}
        for r in pertinents:
            theme = r['analysis']['thematique']
            if theme not in pertinents_by_theme:
                pertinents_by_theme[theme] = []
            pertinents_by_theme[theme].append(r)
        
        for theme, pdfs in list(pertinents_by_theme.items())[:5]:  # Top 5 th√®mes
            print(f"\n   üìÇ {theme} ({len(pdfs)} PDFs):")
            for pdf in pdfs[:3]:  # 3 exemples max
                print(f"      ‚Ä¢ {pdf['title'][:60]}... ({pdf['site_web']})")
        
        # Taille moyenne du texte analys√©
        text_lengths = [r['text_length'] for r in results if 'text_length' in r]
        if text_lengths:
            avg_length = sum(text_lengths) / len(text_lengths)
            print(f"\nüìè Taille moyenne du texte analys√©: {avg_length:.0f} caract√®res")
        
    except Exception as e:
        print(f"‚ùå Erreur analyse fichier r√©sultats: {e}")

def generate_summary_report():
    """
    G√©n√®re un rapport de synth√®se
    """
    print("\nüìã RAPPORT DE SYNTH√àSE")
    print("=" * 50)
    
    # V√©rifier les fichiers g√©n√©r√©s
    files_to_check = [
        'scraper/static/data/pdf_analysis_results.json',
        'pdf_analysis.log'
    ]
    
    print("\nüìÅ Fichiers g√©n√©r√©s:")
    for filename in files_to_check:
        if os.path.exists(filename):
            size = os.path.getsize(filename)
            print(f"   ‚úÖ {filename} ({size:,} octets)")
        else:
            print(f"   ‚ùå {filename} (non trouv√©)")
    
    print("\nüéØ Actions r√©alis√©es:")
    print("   ‚úÖ Suppression des donn√©es scraper_wordsdocument")
    print("   ‚úÖ Mise √† jour des champs site_web")
    print("   ‚úÖ Analyse des PDFs avec Azure OpenAI")
    print("   ‚úÖ G√©n√©ration du fichier de r√©sultats JSON")
    print("   ‚úÖ Logging d√©taill√© des op√©rations")
    
    print("\nüí° Prochaines √©tapes sugg√©r√©es:")
    print("   ‚Ä¢ Examiner les PDFs non pertinents pour affiner les crit√®res")
    print("   ‚Ä¢ Int√©grer les r√©sultats dans l'interface Django")
    print("   ‚Ä¢ Mettre en place un processus d'analyse automatique")
    print("   ‚Ä¢ Cr√©er des filtres par th√©matique et langue")

def main():
    """
    Fonction principale
    """
    print("üîç V√âRIFICATION DE L'ANALYSE DES PDFs")
    print("=" * 60)
    
    # V√©rification de la base de donn√©es
    verify_database_cleanup()
    
    # Analyse des r√©sultats
    analyze_results_file()
    
    # Rapport de synth√®se
    generate_summary_report()
    
    print("\n‚úÖ V√©rification termin√©e")

if __name__ == "__main__":
    main()
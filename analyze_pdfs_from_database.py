#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script d'analyse des PDFs depuis la base de donn√©es

Ce script :
1. Supprime toutes les donn√©es de la table scraper_wordsdocument
2. Met √† jour le champ site_web dans scraper_scrapedpdf avec le nom du site web
3. Analyse les PDFs directement depuis scraper_scrapedpdf avec Azure OpenAI
4. Sauvegarde les r√©sultats dans un fichier JSON

Auteur: Assistant IA
Date: 2025-07-25
"""

import os
import sys
import sqlite3
import json
import logging
from datetime import datetime
from pathlib import Path
import fitz  # PyMuPDF
import requests
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pdf_analysis.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Configuration Azure OpenAI
AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')
AZURE_OPENAI_API_KEY = os.getenv('AZURE_OPENAI_API_KEY')
AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')

# V√©rification des variables d'environnement
if not all([AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, AZURE_OPENAI_DEPLOYMENT_NAME]):
    logger.error("‚ùå Variables d'environnement Azure OpenAI manquantes")
    sys.exit(1)

# Configuration
DATABASE_PATH = 'db.sqlite3'
MAX_PAGES_EXTRACT = 3  # Nombre de pages √† extraire pour l'analyse
MAX_TOKENS = 1000  # Tokens maximum pour OpenAI
RESULT_FILE = 'scraper/static/data/pdf_analysis_results.json'

def azure_openai_chat_completion(prompt):
    """
    Appel √† l'API Azure OpenAI pour l'analyse de texte
    
    Args:
        prompt (str): Le prompt √† envoyer √† OpenAI
    
    Returns:
        str: La r√©ponse de l'API
    """
    url = f"{AZURE_OPENAI_ENDPOINT}/openai/deployments/{AZURE_OPENAI_DEPLOYMENT_NAME}/chat/completions?api-version=2023-05-15"
    headers = {
        "api-key": AZURE_OPENAI_API_KEY,
        "Content-Type": "application/json"
    }
    data = {
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": MAX_TOKENS,
        "temperature": 0.3,
        "top_p": 1,
        "frequency_penalty": 0,
        "presence_penalty": 0
    }

    try:
        response = requests.post(url, headers=headers, json=data, timeout=30)
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except requests.exceptions.RequestException as e:
        logger.error(f"‚ùå Erreur API Azure OpenAI: {e}")
        return None
    except KeyError as e:
        logger.error(f"‚ùå Erreur format r√©ponse OpenAI: {e}")
        return None

def extract_text_from_pdf(file_path, max_pages=3):
    """
    Extrait le texte des premi√®res pages d'un PDF
    
    Args:
        file_path (str): Chemin vers le fichier PDF
        max_pages (int): Nombre maximum de pages √† extraire
    
    Returns:
        str: Texte extrait ou None si erreur
    """
    try:
        if not os.path.exists(file_path):
            logger.warning(f"‚ö†Ô∏è Fichier non trouv√©: {file_path}")
            return None
            
        doc = fitz.open(file_path)
        text_content = ""
        
        # Extraire le texte des premi√®res pages
        pages_to_extract = min(max_pages, len(doc))
        for page_num in range(pages_to_extract):
            page = doc.load_page(page_num)
            text_content += page.get_text()
        
        doc.close()
        
        # Nettoyer le texte
        text_content = text_content.strip()
        if len(text_content) < 50:  # Texte trop court
            return None
            
        return text_content[:4000]  # Limiter la taille pour OpenAI
        
    except Exception as e:
        logger.error(f"‚ùå Erreur extraction PDF {file_path}: {e}")
        return None

def analyze_pdf_with_openai(text_content):
    """
    Analyse le contenu d'un PDF avec Azure OpenAI
    
    Args:
        text_content (str): Contenu textuel du PDF
    
    Returns:
        dict: R√©sultats de l'analyse
    """
    prompt = f"""
Tu es un assistant sp√©cialis√© dans l'analyse de documents institutionnels marocains.

Analyse le texte suivant et fournis une r√©ponse STRICTEMENT au format JSON suivant :

{{
  "pertinent": true/false,
  "langue": "Fran√ßais/Arabe/Anglais/Mixte",
  "thematique": "Agriculture/√âconomie/√âducation/Sant√©/Environnement/Gouvernance/Finance/Industrie/Transport/Autre",
  "confiance": "√âlev√©e/Moyenne/Faible",
  "resume": "R√©sum√© en 2-3 phrases maximum"
}}

Crit√®res de pertinence :
- Document officiel ou institutionnel
- Contenu li√© aux politiques publiques, d√©veloppement, recherche
- Produit par une institution publique ou organisation reconnue
- Utile pour la recherche ou l'analyse des politiques

Texte √† analyser :
{text_content}

R√©ponds UNIQUEMENT avec le JSON, sans texte suppl√©mentaire.
"""

    try:
        response = azure_openai_chat_completion(prompt)
        if not response:
            return {
                "pertinent": False,
                "langue": "Erreur",
                "thematique": "Erreur",
                "confiance": "Faible",
                "resume": "Erreur lors de l'analyse"
            }
        
        # Nettoyer la r√©ponse pour extraire le JSON
        response = response.strip()
        if response.startswith('```json'):
            response = response[7:-3]
        elif response.startswith('```'):
            response = response[3:-3]
        
        # Parser le JSON
        result = json.loads(response)
        
        # Valider les champs requis
        required_fields = ['pertinent', 'langue', 'thematique', 'confiance', 'resume']
        for field in required_fields:
            if field not in result:
                result[field] = "Inconnu"
        
        return result
        
    except json.JSONDecodeError as e:
        logger.error(f"‚ùå Erreur parsing JSON OpenAI: {e}")
        logger.error(f"R√©ponse re√ßue: {response}")
        return {
            "pertinent": False,
            "langue": "Erreur",
            "thematique": "Erreur",
            "confiance": "Faible",
            "resume": "Erreur de format de r√©ponse"
        }
    except Exception as e:
        logger.error(f"‚ùå Erreur analyse OpenAI: {e}")
        return {
            "pertinent": False,
            "langue": "Erreur",
            "thematique": "Erreur",
            "confiance": "Faible",
            "resume": "Erreur lors de l'analyse"
        }

def clean_wordsdocument_table(conn):
    """
    Supprime toutes les donn√©es de la table scraper_wordsdocument
    
    Args:
        conn: Connexion √† la base de donn√©es
    """
    try:
        cursor = conn.cursor()
        
        # V√©rifier si la table existe
        cursor.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name='scraper_wordsdocument'
        """)
        
        if cursor.fetchone():
            # Compter les enregistrements avant suppression
            cursor.execute("SELECT COUNT(*) FROM scraper_wordsdocument")
            count_before = cursor.fetchone()[0]
            
            # Supprimer tous les enregistrements
            cursor.execute("DELETE FROM scraper_wordsdocument")
            conn.commit()
            
            logger.info(f"‚úÖ Table scraper_wordsdocument nettoy√©e: {count_before} enregistrements supprim√©s")
        else:
            logger.info("‚ÑπÔ∏è Table scraper_wordsdocument non trouv√©e")
            
    except Exception as e:
        logger.error(f"‚ùå Erreur nettoyage table wordsdocument: {e}")
        conn.rollback()

def update_site_web_field(conn):
    """
    Met √† jour le champ site_web dans scraper_scrapedpdf avec le nom du site web
    
    Args:
        conn: Connexion √† la base de donn√©es
    """
    try:
        cursor = conn.cursor()
        
        # Mettre √† jour le champ site_web avec le nom du site web
        cursor.execute("""
            UPDATE scraper_scrapedpdf 
            SET site_web = (
                SELECT w.name 
                FROM scraper_website w 
                WHERE w.id = scraper_scrapedpdf.website_id
            )
            WHERE website_id IS NOT NULL
        """)
        
        updated_count = cursor.rowcount
        conn.commit()
        
        logger.info(f"‚úÖ Champ site_web mis √† jour pour {updated_count} enregistrements")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur mise √† jour site_web: {e}")
        conn.rollback()

def load_existing_results(filename=RESULT_FILE):
    """
    Charge les r√©sultats existants depuis le fichier JSON
    
    Args:
        filename (str): Nom du fichier JSON
    
    Returns:
        dict: Dictionnaire des r√©sultats existants avec l'ID comme cl√©
    """
    try:
        if os.path.exists(filename):
            with open(filename, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            # Convertir en dictionnaire avec l'ID comme cl√©
            existing_results = {result['id']: result for result in results}
            logger.info(f"üìÇ {len(existing_results)} r√©sultats existants charg√©s")
            return existing_results
        else:
            logger.info("üìÇ Aucun fichier de r√©sultats existant trouv√©")
            return {}
    except Exception as e:
        logger.error(f"‚ùå Erreur chargement r√©sultats existants: {e}")
        return {}

def get_pdfs_to_analyze(conn, only_new=False):
    """
    R√©cup√®re la liste des PDFs √† analyser depuis la base de donn√©es
    
    Args:
        conn: Connexion √† la base de donn√©es
        only_new (bool): Si True, ne traite que les nouveaux PDFs non analys√©s
    
    Returns:
        list: Liste des PDFs avec leurs m√©tadonn√©es
    """
    try:
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT 
                p.id,
                p.title,
                p.file_path,
                p.url,
                p.site_web,
                w.name as website_name,
                p.downloaded_at
            FROM scraper_scrapedpdf p
            LEFT JOIN scraper_website w ON p.website_id = w.id
            ORDER BY p.downloaded_at DESC
        """)
        
        pdfs = []
        for row in cursor.fetchall():
            pdfs.append({
                'id': row[0],
                'title': row[1],
                'file_path': row[2],
                'url': row[3],
                'site_web': row[4],
                'website_name': row[5],
                'downloaded_at': row[6]
            })
        
        # Si only_new est True, filtrer les PDFs d√©j√† analys√©s
        if only_new:
            existing_results = load_existing_results()
            pdfs = [pdf for pdf in pdfs if pdf['id'] not in existing_results]
            logger.info(f"üìä {len(pdfs)} nouveaux PDFs √† analyser (sur {len(pdfs) + len(existing_results)} total)")
        else:
            logger.info(f"üìä {len(pdfs)} PDFs trouv√©s dans la base de donn√©es")
        
        return pdfs
        
    except Exception as e:
        logger.error(f"‚ùå Erreur r√©cup√©ration PDFs: {e}")
        return []

def save_results_to_json(results, filename=RESULT_FILE, merge_with_existing=False):
    """
    Sauvegarde les r√©sultats dans un fichier JSON
    
    Args:
        results (list): Liste des r√©sultats d'analyse
        filename (str): Nom du fichier de sortie
        merge_with_existing (bool): Si True, fusionne avec les r√©sultats existants
    """
    try:
        # Cr√©er le r√©pertoire si n√©cessaire
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        final_results = results
        
        if merge_with_existing:
            # Charger les r√©sultats existants
            existing_results = load_existing_results(filename)
            
            # Convertir les nouveaux r√©sultats en dictionnaire
            new_results_dict = {result['id']: result for result in results}
            
            # Fusionner (les nouveaux √©crasent les anciens)
            existing_results.update(new_results_dict)
            
            # Convertir en liste
            final_results = list(existing_results.values())
            
            logger.info(f"üìä Fusion: {len(results)} nouveaux + {len(existing_results) - len(results)} existants = {len(final_results)} total")
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"‚úÖ R√©sultats sauvegard√©s dans {filename}")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur sauvegarde JSON: {e}")

def main(only_new=False, skip_cleanup=False):
    """
    Fonction principale
    
    Args:
        only_new (bool): Si True, ne traite que les nouveaux PDFs non analys√©s
        skip_cleanup (bool): Si True, ignore le nettoyage de la table wordsdocument
    """
    mode_text = "nouveaux PDFs" if only_new else "tous les PDFs"
    logger.info(f"üöÄ D√©marrage de l'analyse des {mode_text} depuis la base de donn√©es")
    
    # V√©rifier que la base de donn√©es existe
    if not os.path.exists(DATABASE_PATH):
        logger.error(f"‚ùå Base de donn√©es non trouv√©e: {DATABASE_PATH}")
        return
    
    try:
        # Connexion √† la base de donn√©es
        conn = sqlite3.connect(DATABASE_PATH)
        logger.info("‚úÖ Connexion √† la base de donn√©es √©tablie")
        
        # √âtape 1: Nettoyer la table scraper_wordsdocument (optionnel)
        if not skip_cleanup:
            logger.info("\n=== √âtape 1: Nettoyage de la table scraper_wordsdocument ===")
            clean_wordsdocument_table(conn)
        else:
            logger.info("\n=== √âtape 1: Nettoyage ignor√© ===")
        
        # √âtape 2: Mettre √† jour le champ site_web
        logger.info("\n=== √âtape 2: Mise √† jour du champ site_web ===")
        update_site_web_field(conn)
        
        # √âtape 3: R√©cup√©rer les PDFs √† analyser
        logger.info("\n=== √âtape 3: R√©cup√©ration des PDFs ===")
        pdfs = get_pdfs_to_analyze(conn, only_new=only_new)
        
        if not pdfs:
            if only_new:
                logger.info("‚úÖ Aucun nouveau PDF √† analyser")
            else:
                logger.warning("‚ö†Ô∏è Aucun PDF trouv√© dans la base de donn√©es")
            return
        
        # √âtape 4: Analyser les PDFs
        logger.info(f"\n=== √âtape 4: Analyse de {len(pdfs)} PDFs ===")
        results = []
        processed = 0
        errors = 0
        
        for i, pdf in enumerate(pdfs, 1):
            logger.info(f"\nüìÑ [{i}/{len(pdfs)}] Analyse: {pdf['title']}")
            
            # Construire le chemin complet du fichier
            file_path = pdf['file_path']
            if not os.path.isabs(file_path):
                # Si le chemin n'est pas absolu, le construire
                file_path = os.path.join(os.getcwd(), file_path)
            
            # Extraire le texte du PDF
            text_content = extract_text_from_pdf(file_path, MAX_PAGES_EXTRACT)
            
            if not text_content:
                logger.warning(f"‚ö†Ô∏è Impossible d'extraire le texte de {pdf['title']}")
                errors += 1
                continue
            
            # Analyser avec OpenAI
            analysis = analyze_pdf_with_openai(text_content)
            
            # Pr√©parer le r√©sultat
            result = {
                'id': pdf['id'],
                'title': pdf['title'],
                'file_path': pdf['file_path'],
                'url': pdf['url'],
                'site_web': pdf['site_web'] or pdf['website_name'],
                'downloaded_at': pdf['downloaded_at'],
                'analysis': analysis,
                'text_length': len(text_content),
                'analyzed_at': datetime.now().isoformat()
            }
            
            results.append(result)
            processed += 1
            
            # Afficher le r√©sultat
            if analysis['pertinent']:
                logger.info(f"‚úÖ PDF pertinent - Langue: {analysis['langue']}, Th√®me: {analysis['thematique']}")
            else:
                logger.info(f"‚õî PDF non pertinent")
        
        # √âtape 5: Sauvegarder les r√©sultats
        logger.info("\n=== √âtape 5: Sauvegarde des r√©sultats ===")
        save_results_to_json(results, merge_with_existing=only_new)
        
        # Statistiques finales
        pertinents = sum(1 for r in results if r['analysis']['pertinent'])
        logger.info(f"\n=== R√âSUM√â ===")
        logger.info(f"üìä PDFs trait√©s: {processed}")
        logger.info(f"‚úÖ PDFs pertinents: {pertinents}")
        logger.info(f"‚õî PDFs non pertinents: {processed - pertinents}")
        logger.info(f"‚ùå Erreurs: {errors}")
        logger.info(f"üìÅ R√©sultats sauvegard√©s dans: {RESULT_FILE}")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur g√©n√©rale: {e}")
    finally:
        if 'conn' in locals():
            conn.close()
            logger.info("üîí Connexion √† la base de donn√©es ferm√©e")

if __name__ == "__main__":
    main()
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Pipeline Intelligent de Traitement PDF

Ce script automatise le processus complet de traitement des PDFs :
1. Scraping intelligent (√©vite les doublons)
2. Insertion intelligente en base de donn√©es
3. Analyse OpenAI des PDFs non analys√©s
4. D√©marrage automatique du serveur Django

Auteur: Assistant IA
Date: 2025-01-27
"""

import os
import sys
import subprocess
import time
import sqlite3
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Set, Dict, Optional

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('intelligent_pipeline.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class IntelligentPipeline:
    """
    Pipeline intelligent pour le traitement automatis√© des PDFs
    """
    
    def __init__(self):
        self.db_path = 'db.sqlite3'
        self.result_file = 'scraper/static/data/pdf_analysis_results.json'
        self.sites = [
            'agriculture.gov.ma',
            'bkam.ma', 
            'cese.ma',
            'finances.gov.ma',
            'oecd.org'
        ]
        
    def print_header(self):
        """Affiche l'en-t√™te du pipeline"""
        print("\n" + "="*60)
        print("üöÄ PIPELINE INTELLIGENT DE TRAITEMENT PDF")
        print("="*60)
        print("üìã Fonctionnalit√©s intelligentes :")
        print("   üß† Scraping intelligent (√©vite les doublons)")
        print("   üß† Insertion intelligente en base de donn√©es")
        print("   üß† Analyse OpenAI des PDFs non analys√©s uniquement")
        print("   üß† D√©marrage automatique du serveur Django")
        print("="*60)
        
    def get_existing_files_from_db(self) -> Set[str]:
        """
        R√©cup√®re les fichiers d√©j√† pr√©sents dans la base de donn√©es
        
        Returns:
            Set[str]: Ensemble des noms de fichiers d√©j√† scrap√©s
        """
        try:
            if not os.path.exists(self.db_path):
                logger.info("üìä Base de donn√©es non trouv√©e - tous les fichiers seront scrap√©s")
                return set()
                
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT DISTINCT file_path 
                FROM scraper_scrapedpdf
            """)
            
            existing_files = set()
            for row in cursor.fetchall():
                file_path = row[0]
                filename = os.path.basename(file_path)
                existing_files.add(filename)
            
            conn.close()
            logger.info(f"üìä {len(existing_files)} fichiers d√©j√† pr√©sents dans la base de donn√©es")
            return existing_files
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la lecture de la base de donn√©es: {e}")
            return set()
    
    def get_analyzed_pdfs(self) -> Set[int]:
        """
        R√©cup√®re les IDs des PDFs d√©j√† analys√©s
        
        Returns:
            Set[int]: Ensemble des IDs des PDFs d√©j√† analys√©s
        """
        try:
            if not os.path.exists(self.result_file):
                logger.info("üìä Aucun fichier de r√©sultats d'analyse trouv√© - tous les PDFs seront analys√©s")
                return set()
                
            with open(self.result_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            analyzed_ids = {result['id'] for result in results}
            logger.info(f"üìä {len(analyzed_ids)} PDFs d√©j√† analys√©s")
            return analyzed_ids
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la lecture des r√©sultats d'analyse: {e}")
            return set()
    
    def check_files_in_directory(self, directory: str) -> List[str]:
        """
        R√©cup√®re la liste des fichiers PDF dans un r√©pertoire
        
        Args:
            directory (str): Chemin du r√©pertoire
        
        Returns:
            List[str]: Liste des fichiers PDF
        """
        pdf_files = []
        
        if not os.path.exists(directory):
            return pdf_files
        
        try:
            for root, dirs, files in os.walk(directory):
                for file in files:
                    if file.lower().endswith('.pdf'):
                        pdf_files.append(file)
            return pdf_files
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la lecture du r√©pertoire {directory}: {e}")
            return pdf_files
    
    def check_scraping_needed(self) -> bool:
        """
        V√©rifie si le scraping est n√©cessaire
        
        Returns:
            bool: True si scraping n√©cessaire
        """
        logger.info("\nüîç V√âRIFICATION DE LA N√âCESSIT√â DU SCRAPING")
        logger.info("-" * 50)
        
        existing_files = self.get_existing_files_from_db()
        new_files_found = False
        
        for site in self.sites:
            pdf_downloads_dir = os.path.join(site, 'pdf_downloads')
            current_files = set(self.check_files_in_directory(pdf_downloads_dir))
            
            # Fichiers qui ne sont pas encore en base
            new_files = current_files - existing_files
            
            if new_files:
                logger.info(f"üÜï {len(new_files)} nouveaux fichiers d√©tect√©s pour {site}")
                for file in sorted(list(new_files)[:3]):  # Afficher max 3 exemples
                    logger.info(f"   - {file}")
                if len(new_files) > 3:
                    logger.info(f"   ... et {len(new_files) - 3} autres")
                new_files_found = True
            else:
                logger.info(f"‚úÖ Aucun nouveau fichier pour {site}")
        
        return new_files_found
    
    def check_analysis_needed(self) -> bool:
        """
        V√©rifie si l'analyse est n√©cessaire
        
        Returns:
            bool: True si analyse n√©cessaire
        """
        logger.info("\nüîç V√âRIFICATION DE LA N√âCESSIT√â DE L'ANALYSE")
        logger.info("-" * 50)
        
        try:
            if not os.path.exists(self.db_path):
                logger.info("üìä Base de donn√©es non trouv√©e - analyse non n√©cessaire")
                return False
                
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Compter le total de PDFs en base
            cursor.execute("SELECT COUNT(*) FROM scraper_scrapedpdf")
            total_pdfs = cursor.fetchone()[0]
            
            conn.close()
            
            # Compter les PDFs d√©j√† analys√©s
            analyzed_pdfs = self.get_analyzed_pdfs()
            
            unanalyzed_count = total_pdfs - len(analyzed_pdfs)
            
            logger.info(f"üìä PDFs en base de donn√©es: {total_pdfs}")
            logger.info(f"üìä PDFs d√©j√† analys√©s: {len(analyzed_pdfs)}")
            logger.info(f"üìä PDFs √† analyser: {unanalyzed_count}")
            
            return unanalyzed_count > 0
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la v√©rification de l'analyse: {e}")
            return False
    
    def run_intelligent_scraping(self) -> bool:
        """
        Ex√©cute le scraping intelligent
        
        Returns:
            bool: True si succ√®s
        """
        logger.info("\n" + "="*50)
        logger.info("üîÑ √âTAPE 1: SCRAPING INTELLIGENT")
        logger.info("="*50)
        
        if not self.check_scraping_needed():
            logger.info("‚úÖ Aucun nouveau fichier d√©tect√© - scraping ignor√©")
            return True
        
        try:
            logger.info("üöÄ D√©marrage du scraping intelligent...")
            
            result = subprocess.run(
                ['python', 'intelligent_scraper.py'],
                check=True,
                capture_output=True,
                text=True,
                timeout=1800  # 30 minutes max
            )
            
            logger.info("‚úÖ Scraping intelligent termin√© avec succ√®s")
            
            # Afficher la sortie si elle existe
            if result.stdout.strip():
                for line in result.stdout.strip().split('\n')[-10:]:  # Derni√®res 10 lignes
                    logger.info(f"   {line}")
            
            return True
            
        except subprocess.TimeoutExpired:
            logger.error("‚ùå Timeout du scraping intelligent (>30 min)")
            return False
            
        except subprocess.CalledProcessError as e:
            logger.error(f"‚ùå Erreur lors du scraping intelligent:")
            logger.error(f"   Code de retour: {e.returncode}")
            
            if e.stderr:
                logger.error(f"   Erreur: {e.stderr.strip()}")
            
            return False
            
        except Exception as e:
            logger.error(f"‚ùå Erreur inattendue lors du scraping: {e}")
            return False
    
    def run_database_reorganization(self) -> bool:
        """
        Ex√©cute la r√©organisation de la base de donn√©es
        
        Returns:
            bool: True si succ√®s
        """
        logger.info("\n" + "="*50)
        logger.info("üîÑ √âTAPE 2: R√âORGANISATION DE LA BASE DE DONN√âES")
        logger.info("="*50)
        
        try:
            logger.info("üöÄ D√©marrage de la r√©organisation de la base de donn√©es...")
            
            result = subprocess.run(
                ['python', 'reorganize_database.py'],
                check=True,
                capture_output=True,
                text=True,
                timeout=300  # 5 minutes max
            )
            
            logger.info("‚úÖ R√©organisation de la base de donn√©es termin√©e avec succ√®s")
            
            # Afficher la sortie si elle existe
            if result.stdout.strip():
                for line in result.stdout.strip().split('\n')[-5:]:  # Derni√®res 5 lignes
                    logger.info(f"   {line}")
            
            return True
            
        except subprocess.TimeoutExpired:
            logger.error("‚ùå Timeout de la r√©organisation (>5 min)")
            return False
            
        except subprocess.CalledProcessError as e:
            logger.error(f"‚ùå Erreur lors de la r√©organisation:")
            logger.error(f"   Code de retour: {e.returncode}")
            
            if e.stderr:
                logger.error(f"   Erreur: {e.stderr.strip()}")
            
            return False
            
        except Exception as e:
            logger.error(f"‚ùå Erreur inattendue lors de la r√©organisation: {e}")
            return False
    
    def run_intelligent_analysis(self) -> bool:
        """
        Ex√©cute l'analyse intelligente des PDFs
        
        Returns:
            bool: True si succ√®s
        """
        logger.info("\n" + "="*50)
        logger.info("üîÑ √âTAPE 3: ANALYSE INTELLIGENTE DES PDFs")
        logger.info("="*50)
        
        if not self.check_analysis_needed():
            logger.info("‚úÖ Aucun nouveau PDF √† analyser - analyse ignor√©e")
            return True
        
        try:
            logger.info("üöÄ D√©marrage de l'analyse intelligente des PDFs...")
            
            # Utiliser le mode only_new pour ne traiter que les nouveaux PDFs
            result = subprocess.run(
                ['python', '-c', 
                 'import analyze_pdfs_from_database; analyze_pdfs_from_database.main(only_new=True, skip_cleanup=True)'],
                check=True,
                capture_output=True,
                text=True,
                timeout=3600  # 60 minutes max
            )
            
            logger.info("‚úÖ Analyse intelligente des PDFs termin√©e avec succ√®s")
            
            # Afficher la sortie si elle existe
            if result.stdout.strip():
                for line in result.stdout.strip().split('\n')[-10:]:  # Derni√®res 10 lignes
                    logger.info(f"   {line}")
            
            return True
            
        except subprocess.TimeoutExpired:
            logger.error("‚ùå Timeout de l'analyse (>60 min)")
            return False
            
        except subprocess.CalledProcessError as e:
            logger.error(f"‚ùå Erreur lors de l'analyse:")
            logger.error(f"   Code de retour: {e.returncode}")
            
            if e.stderr:
                logger.error(f"   Erreur: {e.stderr.strip()}")
            
            return False
            
        except Exception as e:
            logger.error(f"‚ùå Erreur inattendue lors de l'analyse: {e}")
            return False
    
    def start_django_server(self) -> bool:
        """
        D√©marre le serveur Django
        
        Returns:
            bool: True si succ√®s
        """
        logger.info("\n" + "="*50)
        logger.info("üîÑ √âTAPE 4: D√âMARRAGE DU SERVEUR DJANGO")
        logger.info("="*50)
        
        try:
            logger.info("üöÄ D√©marrage du serveur Django...")
            
            # D√©marrer le serveur en arri√®re-plan
            process = subprocess.Popen(
                ['python', 'manage.py', 'runserver'],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # Attendre un peu pour v√©rifier que le serveur d√©marre
            time.sleep(3)
            
            if process.poll() is None:
                logger.info("‚úÖ Serveur Django d√©marr√© avec succ√®s !")
                logger.info("üåê Le serveur est maintenant actif.")
                logger.info("üîó Vous pouvez acc√©der √† l'interface √† : http://127.0.0.1:8000/")
                logger.info("‚ö†Ô∏è  Utilisez Ctrl+C pour arr√™ter le serveur.")
                
                # Attendre que l'utilisateur arr√™te le serveur
                try:
                    process.wait()
                except KeyboardInterrupt:
                    logger.info("\n‚èπÔ∏è  Arr√™t du serveur Django demand√© par l'utilisateur...")
                    process.terminate()
                    try:
                        process.wait(timeout=5)
                    except subprocess.TimeoutExpired:
                        process.kill()
                    logger.info("‚úÖ Serveur Django arr√™t√©")
                
                return True
            else:
                logger.error("‚ùå Le serveur Django n'a pas pu d√©marrer")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du d√©marrage du serveur Django: {e}")
            return False
    
    def run_pipeline(self):
        """
        Ex√©cute le pipeline complet
        """
        self.print_header()
        
        start_time = datetime.now()
        steps_completed = 0
        total_steps = 4
        
        try:
            # √âtape 1: Scraping intelligent
            if self.run_intelligent_scraping():
                steps_completed += 1
                logger.info("‚úÖ √âtape 1 termin√©e avec succ√®s")
            else:
                logger.warning("‚ö†Ô∏è  √âtape 1 √©chou√©e, mais continuation du pipeline...")
            
            time.sleep(2)  # Pause entre les √©tapes
            
            # √âtape 2: R√©organisation de la base de donn√©es
            if self.run_database_reorganization():
                steps_completed += 1
                logger.info("‚úÖ √âtape 2 termin√©e avec succ√®s")
            else:
                logger.warning("‚ö†Ô∏è  √âtape 2 √©chou√©e, mais continuation du pipeline...")
            
            time.sleep(2)  # Pause entre les √©tapes
            
            # √âtape 3: Analyse intelligente
            if self.run_intelligent_analysis():
                steps_completed += 1
                logger.info("‚úÖ √âtape 3 termin√©e avec succ√®s")
            else:
                logger.warning("‚ö†Ô∏è  √âtape 3 √©chou√©e, mais continuation du pipeline...")
            
            time.sleep(2)  # Pause entre les √©tapes
            
            # √âtape 4: D√©marrage du serveur Django
            if self.start_django_server():
                steps_completed += 1
                logger.info("‚úÖ √âtape 4 termin√©e avec succ√®s")
            else:
                logger.error("‚ùå √âtape 4 √©chou√©e")
            
        except KeyboardInterrupt:
            logger.info("\n\n‚ö†Ô∏è  Pipeline interrompu par l'utilisateur (Ctrl+C)")
        
        except Exception as e:
            logger.error(f"\n\n‚ùå ERREUR CRITIQUE DU PIPELINE !")
            logger.error(f"   Type: {type(e).__name__}")
            logger.error(f"   Message: {str(e)}")
        
        finally:
            # R√©sum√© final
            end_time = datetime.now()
            duration = end_time - start_time
            
            logger.info("\n" + "="*60)
            logger.info("üìä R√âSUM√â DU PIPELINE INTELLIGENT")
            logger.info("="*60)
            logger.info(f"‚úÖ √âtapes r√©ussies : {steps_completed}/{total_steps}")
            logger.info(f"‚è∞ Dur√©e totale : {duration}")
            
            if steps_completed == total_steps:
                logger.info("üéâ PIPELINE TERMIN√â AVEC SUCC√àS !")
                logger.info("   Tous les traitements ont √©t√© effectu√©s intelligemment.")
            elif steps_completed >= 2:
                logger.info("‚ö†Ô∏è  PIPELINE PARTIELLEMENT R√âUSSI")
                logger.info(f"   {total_steps - steps_completed} √©tape(s) ont √©chou√©.")
            else:
                logger.info("‚ùå PIPELINE √âCHOU√â")
                logger.info("   La plupart des √©tapes ont √©chou√©.")
            
            logger.info(f"\n‚è∞ Pipeline termin√© √† {end_time.strftime('%H:%M:%S')}")
            logger.info("="*60)

def main():
    """
    Fonction principale
    """
    pipeline = IntelligentPipeline()
    pipeline.run_pipeline()

if __name__ == '__main__':
    main()
